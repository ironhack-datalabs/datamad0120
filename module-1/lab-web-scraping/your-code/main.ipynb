{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Lab\n",
    "\n",
    "You will find in this notebook some scrapy exercises to practise your scraping skills.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "- Check the response status code for each request to ensure you have obtained the intended contennt.\n",
    "- Print the response text in each request to understand the kind of info you are getting and its format.\n",
    "- Check for patterns in the response text to extract the data/info requested in each question.\n",
    "- Visit each url and take a look at its source through Chrome DevTools. You'll need to identify the html tags, special class names etc. used for the html content you are expected to extract."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide) documentation \n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [lxml lib](https://lxml.de/)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are the libraries and modules you may need. `requests`,  `BeautifulSoup` and `pandas` are imported for you. If you prefer to use additional libraries feel free to uncomment them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "# from pprint import pprint\n",
    "# from lxml import html\n",
    "# from lxml.html import fromstring\n",
    "# import urllib.request\n",
    "# from urllib.request import urlopen\n",
    "# import random\n",
    "# import re\n",
    "# import scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "res = requests.get(url)\n",
    "soup = BeautifulSoup(res.text, 'html.parser')\n",
    "#print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the names of the trending developers retrieved in the previous step.\n",
    "\n",
    "Your output should be a Python list of developer names. Each name should not contain any html tag.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Find out the html tag and class names used for the developer names. You can achieve this using Chrome DevTools.\n",
    "\n",
    "1. Use BeautifulSoup to extract all the html elements that contain the developer names.\n",
    "\n",
    "1. Use string manipulation techniques to replace whitespaces and linebreaks (i.e. `\\n`) in the *text* of each html element. Use a list to store the clean names.\n",
    "\n",
    "1. Print the list of names.\n",
    "\n",
    "Your output should look like below:\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (神楽坂覚々)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mattn\n"
     ]
    }
   ],
   "source": [
    "#your code\n",
    "develop = soup.find_all(class_ = 'h3')[0].text\n",
    "print(develop.strip())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mattn mattn',\n",
       " 'Andreas Mueller amueller',\n",
       " 'Brian C brianc',\n",
       " 'Erik Schierboom ErikSchierboom',\n",
       " 'John Sundell JohnSundell',\n",
       " 'Siddharth Kshetrapal siddharthkp',\n",
       " 'Jason Williams jasonwilliams',\n",
       " 'Kyle Conroy kyleconroy',\n",
       " 'Anthony Sottile asottile',\n",
       " 'William Candillon wcandillon',\n",
       " 'Colin Rofls cmyr',\n",
       " 'Filip Skokan panva',\n",
       " 'Amila Welihinda amilajack',\n",
       " 'Kristoffer Carlsson KristofferC',\n",
       " 'Giulio Canti gcanti',\n",
       " 'greatghoul greatghoul',\n",
       " 'Miek Gieben miekg',\n",
       " 'Jesse Katsumata Naturalclar',\n",
       " 'Lucas Michot lucasmichot',\n",
       " 'Adam Geitgey ageitgey',\n",
       " 'Philipp Oppermann phil-opp',\n",
       " 'Tony Narlock tony',\n",
       " 'isaacs isaacs',\n",
       " 'Florian Rival 4ian',\n",
       " 'Satyajit Sahoo satya164']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def processDev(tag):\n",
    "    try:\n",
    "        a = (tag.select('h1[class^=\"h3\"]')[0].text).strip()\n",
    "        b = (tag.select('p[class^=\"f4\"]')[0].text).strip()\n",
    "        c = (a, b)\n",
    "        return ' '.join(c)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "rests = [processDev(tag) for tag in soup.select('div[class^=\"col-md-6\"]')]\n",
    "rests\n",
    "list(filter(lambda x: x,rests))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the trending Python repositories in GitHub\n",
    "\n",
    "The steps to solve this problem is similar to the previous one except that you need to find out the repository names instead of developer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/python?since=daily'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(url)\n",
    "soup = BeautifulSoup(res.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BlankerL / DXY-2019-nCoV-Crawler',\n",
       " 'gto76 / python-cheatsheet',\n",
       " 'smicallef / spiderfoot',\n",
       " 'sebastianruder / NLP-progress',\n",
       " 'rish-16 / sight',\n",
       " 'luong-komorebi / Awesome-Linux-Software',\n",
       " 'vinta / awesome-python',\n",
       " 'nicrusso7 / rex-gym',\n",
       " 'baowenbo / DAIN',\n",
       " 'eastlakeside / interpy-zh',\n",
       " 'tiangolo / fastapi',\n",
       " 'explosion / thinc',\n",
       " 'willmcgugan / rich',\n",
       " 'MatthewPierson / Vieux',\n",
       " 'CCExtractor / vardbg',\n",
       " 'dragen1860 / Deep-Learning-with-TensorFlow-book',\n",
       " 'brendangregg / bpf-perf-tools-book',\n",
       " 'googleapis / google-cloud-python',\n",
       " 'anchore / anchore-engine',\n",
       " 'aj-4 / tinder-swipe-bot',\n",
       " 'pytorch / captum',\n",
       " 'samuelhwilliams / Eel',\n",
       " 'salesforce / policy_sentry',\n",
       " 'yzhao062 / anomaly-detection-resources',\n",
       " 'vineetjohn / daily-coding-problem']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "def processRepo(tag):\n",
    "    try:  \n",
    "        return ' '.join((tag.select('a')[1].text).split())\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "rests = [processRepo(tag) for tag in soup.select('article[class^=\"Box-row\"]')]\n",
    "rests = list(filter(lambda x: x, rests))\n",
    "rests\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display all the image links from Walt Disney wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/Walt_Disney'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(url)\n",
    "soup = BeautifulSoup(res.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['//upload.wikimedia.org/wikipedia/en/thumb/e/e7/Cscr-featured.svg/20px-Cscr-featured.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/1/1b/Semi-protection-shackle.svg/20px-Semi-protection-shackle.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/d/df/Walt_Disney_1946.JPG/220px-Walt_Disney_1946.JPG',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/8/87/Walt_Disney_1942_signature.svg/150px-Walt_Disney_1942_signature.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/c/c4/Walt_Disney_envelope_ca._1921.jpg/220px-Walt_Disney_envelope_ca._1921.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/4/4d/Newman_Laugh-O-Gram_%281921%29.webm/220px-seek%3D2-Newman_Laugh-O-Gram_%281921%29.webm.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/0/0d/Trolley_Troubles_poster.jpg/170px-Trolley_Troubles_poster.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/7/71/Walt_Disney_and_his_cartoon_creation_%22Mickey_Mouse%22_-_National_Board_of_Review_Magazine.jpg/170px-Walt_Disney_and_his_cartoon_creation_%22Mickey_Mouse%22_-_National_Board_of_Review_Magazine.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/4/4e/Steamboat-willie.jpg/170px-Steamboat-willie.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/5/57/Walt_Disney_1935.jpg/170px-Walt_Disney_1935.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/c/cd/Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg/220px-Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/1/15/Disney_drawing_goofy.jpg/170px-Disney_drawing_goofy.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/1/13/DisneySchiphol1951.jpg/220px-DisneySchiphol1951.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/8/8c/WaltDisneyplansDisneylandDec1954.jpg/220px-WaltDisneyplansDisneylandDec1954.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Walt_disney_portrait_right.jpg/170px-Walt_disney_portrait_right.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Walt_Disney_Grave.JPG/170px-Walt_Disney_Grave.JPG',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/2/2d/Roy_O._Disney_with_Company_at_Press_Conference.jpg/170px-Roy_O._Disney_with_Company_at_Press_Conference.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/a/a9/Disney_Display_Case.JPG/170px-Disney_Display_Case.JPG',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Disney1968.jpg/170px-Disney1968.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/4/44/The_Walt_Disney_Company_Logo.svg/120px-The_Walt_Disney_Company_Logo.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/d/da/Animation_disc.svg/30px-Animation_disc.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/6/69/P_vip.svg/29px-P_vip.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Magic_Kingdom_castle.jpg/24px-Magic_Kingdom_castle.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/e/e7/Video-x-generic.svg/30px-Video-x-generic.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/a/a3/Flag_of_Los_Angeles_County%2C_California.svg/30px-Flag_of_Los_Angeles_County%2C_California.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Blank_television_set.svg/30px-Blank_television_set.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/a/a4/Flag_of_the_United_States.svg/30px-Flag_of_the_United_States.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/22px-Commons-logo.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Wikiquote-logo.svg/25px-Wikiquote-logo.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Wikidata-logo.svg/30px-Wikidata-logo.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png',\n",
       " '//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1',\n",
       " '/static/images/wikimedia-button.png',\n",
       " '/static/images/poweredby_mediawiki_88x31.png']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "def processWalt(tag):\n",
    "    try:  \n",
    "        return tag['src']\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "rests = [processWalt(tag) for tag in soup.select('img')]\n",
    "rests = list(filter(lambda x: x, rests))\n",
    "rests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve an arbitary Wikipedia page of \"Python\" and create a list of links on that page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://en.wikipedia.org/wiki/Python' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(url)\n",
    "soup = BeautifulSoup(res.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#mw-head',\n",
       " '#p-search',\n",
       " 'https://en.wiktionary.org/wiki/Python',\n",
       " 'https://en.wiktionary.org/wiki/python',\n",
       " '#Snakes',\n",
       " '#Ancient_Greece',\n",
       " '#Media_and_entertainment',\n",
       " '#Computing',\n",
       " '#Engineering',\n",
       " '#Roller_coasters',\n",
       " '#Vehicles',\n",
       " '#Weaponry',\n",
       " '#People',\n",
       " '#Other_uses',\n",
       " '#See_also',\n",
       " '/w/index.php?title=Python&action=edit&section=1',\n",
       " '/wiki/Pythonidae',\n",
       " '/wiki/Python_(genus)',\n",
       " '/w/index.php?title=Python&action=edit&section=2',\n",
       " '/wiki/Python_(mythology)',\n",
       " '/wiki/Python_of_Aenus',\n",
       " '/wiki/Python_(painter)',\n",
       " '/wiki/Python_of_Byzantium',\n",
       " '/wiki/Python_of_Catana',\n",
       " '/w/index.php?title=Python&action=edit&section=3',\n",
       " '/wiki/Python_(film)',\n",
       " '/wiki/Pythons_2',\n",
       " '/wiki/Monty_Python',\n",
       " '/wiki/Python_(Monty)_Pictures',\n",
       " '/w/index.php?title=Python&action=edit&section=4',\n",
       " '/wiki/Python_(programming_language)',\n",
       " '/wiki/CPython',\n",
       " '/wiki/CMU_Common_Lisp',\n",
       " '/wiki/PERQ#PERQ_3',\n",
       " '/w/index.php?title=Python&action=edit&section=5',\n",
       " '/w/index.php?title=Python&action=edit&section=6',\n",
       " '/wiki/Python_(Busch_Gardens_Tampa_Bay)',\n",
       " '/wiki/Python_(Coney_Island,_Cincinnati,_Ohio)',\n",
       " '/wiki/Python_(Efteling)',\n",
       " '/w/index.php?title=Python&action=edit&section=7',\n",
       " '/wiki/Python_(automobile_maker)',\n",
       " '/wiki/Python_(Ford_prototype)',\n",
       " '/w/index.php?title=Python&action=edit&section=8',\n",
       " '/wiki/Colt_Python',\n",
       " '/wiki/Python_(missile)',\n",
       " '/wiki/Python_(nuclear_primary)',\n",
       " '/w/index.php?title=Python&action=edit&section=9',\n",
       " '/wiki/Python_Anghelo',\n",
       " '/w/index.php?title=Python&action=edit&section=10',\n",
       " '/wiki/PYTHON',\n",
       " '/w/index.php?title=Python&action=edit&section=11',\n",
       " '/wiki/Cython',\n",
       " '/wiki/Pyton',\n",
       " '/wiki/File:Disambig_gray.svg',\n",
       " '/wiki/Help:Disambiguation',\n",
       " 'https://en.wikipedia.org/w/index.php?title=Special:WhatLinksHere/Python&namespace=0',\n",
       " 'https://en.wikipedia.org/w/index.php?title=Python&oldid=937840393',\n",
       " '/wiki/Help:Category',\n",
       " '/wiki/Category:Disambiguation_pages',\n",
       " '/wiki/Category:Disambiguation_pages_with_short_description',\n",
       " '/wiki/Category:All_article_disambiguation_pages',\n",
       " '/wiki/Category:All_disambiguation_pages',\n",
       " '/wiki/Category:Animal_common_name_disambiguation_pages',\n",
       " '/wiki/Special:MyTalk',\n",
       " '/wiki/Special:MyContributions',\n",
       " '/w/index.php?title=Special:CreateAccount&returnto=Python',\n",
       " '/w/index.php?title=Special:UserLogin&returnto=Python',\n",
       " '/wiki/Python',\n",
       " '/wiki/Talk:Python',\n",
       " '/wiki/Python',\n",
       " '/w/index.php?title=Python&action=edit',\n",
       " '/w/index.php?title=Python&action=history',\n",
       " '/wiki/Main_Page',\n",
       " '/wiki/Main_Page',\n",
       " '/wiki/Wikipedia:Contents',\n",
       " '/wiki/Wikipedia:Featured_content',\n",
       " '/wiki/Portal:Current_events',\n",
       " '/wiki/Special:Random',\n",
       " 'https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&utm_medium=sidebar&utm_campaign=C13_en.wikipedia.org&uselang=en',\n",
       " '//shop.wikimedia.org',\n",
       " '/wiki/Help:Contents',\n",
       " '/wiki/Wikipedia:About',\n",
       " '/wiki/Wikipedia:Community_portal',\n",
       " '/wiki/Special:RecentChanges',\n",
       " '//en.wikipedia.org/wiki/Wikipedia:Contact_us',\n",
       " '/wiki/Special:WhatLinksHere/Python',\n",
       " '/wiki/Special:RecentChangesLinked/Python',\n",
       " '/wiki/Wikipedia:File_Upload_Wizard',\n",
       " '/wiki/Special:SpecialPages',\n",
       " '/w/index.php?title=Python&oldid=937840393',\n",
       " '/w/index.php?title=Python&action=info',\n",
       " 'https://www.wikidata.org/wiki/Special:EntityPage/Q747452',\n",
       " '/w/index.php?title=Special:CiteThisPage&page=Python&id=937840393',\n",
       " 'https://commons.wikimedia.org/wiki/Category:Python',\n",
       " '/w/index.php?title=Special:Book&bookcmd=book_creator&referer=Python',\n",
       " '/w/index.php?title=Special:ElectronPdf&page=Python&action=show-download-screen',\n",
       " '/w/index.php?title=Python&printable=yes',\n",
       " 'https://af.wikipedia.org/wiki/Python',\n",
       " 'https://als.wikipedia.org/wiki/Python',\n",
       " 'https://az.wikipedia.org/wiki/Python',\n",
       " 'https://bn.wikipedia.org/wiki/%E0%A6%AA%E0%A6%BE%E0%A6%87%E0%A6%A5%E0%A6%A8_(%E0%A6%A6%E0%A7%8D%E0%A6%AC%E0%A7%8D%E0%A6%AF%E0%A6%B0%E0%A7%8D%E0%A6%A5%E0%A6%A4%E0%A6%BE_%E0%A6%A8%E0%A6%BF%E0%A6%B0%E0%A6%B8%E0%A6%A8)',\n",
       " 'https://be.wikipedia.org/wiki/Python',\n",
       " 'https://bg.wikipedia.org/wiki/%D0%9F%D0%B8%D1%82%D0%BE%D0%BD_(%D0%BF%D0%BE%D1%8F%D1%81%D0%BD%D0%B5%D0%BD%D0%B8%D0%B5)',\n",
       " 'https://cs.wikipedia.org/wiki/Python_(rozcestn%C3%ADk)',\n",
       " 'https://da.wikipedia.org/wiki/Python',\n",
       " 'https://de.wikipedia.org/wiki/Python',\n",
       " 'https://eo.wikipedia.org/wiki/Pitono_(apartigilo)',\n",
       " 'https://eu.wikipedia.org/wiki/Python_(argipena)',\n",
       " 'https://fa.wikipedia.org/wiki/%D9%BE%D8%A7%DB%8C%D8%AA%D9%88%D9%86',\n",
       " 'https://fr.wikipedia.org/wiki/Python',\n",
       " 'https://ko.wikipedia.org/wiki/%ED%8C%8C%EC%9D%B4%EC%84%A0',\n",
       " 'https://hr.wikipedia.org/wiki/Python_(razdvojba)',\n",
       " 'https://io.wikipedia.org/wiki/Pitono',\n",
       " 'https://id.wikipedia.org/wiki/Python',\n",
       " 'https://ia.wikipedia.org/wiki/Python_(disambiguation)',\n",
       " 'https://is.wikipedia.org/wiki/Python_(a%C3%B0greining)',\n",
       " 'https://it.wikipedia.org/wiki/Python_(disambigua)',\n",
       " 'https://he.wikipedia.org/wiki/%D7%A4%D7%99%D7%AA%D7%95%D7%9F',\n",
       " 'https://ka.wikipedia.org/wiki/%E1%83%9E%E1%83%98%E1%83%97%E1%83%9D%E1%83%9C%E1%83%98_(%E1%83%9B%E1%83%A0%E1%83%90%E1%83%95%E1%83%90%E1%83%9A%E1%83%9B%E1%83%9C%E1%83%98%E1%83%A8%E1%83%95%E1%83%9C%E1%83%94%E1%83%9A%E1%83%9D%E1%83%95%E1%83%90%E1%83%9C%E1%83%98)',\n",
       " 'https://kg.wikipedia.org/wiki/Mboma_(nyoka)',\n",
       " 'https://la.wikipedia.org/wiki/Python_(discretiva)',\n",
       " 'https://lb.wikipedia.org/wiki/Python',\n",
       " 'https://hu.wikipedia.org/wiki/Python_(egy%C3%A9rtelm%C5%B1s%C3%ADt%C5%91_lap)',\n",
       " 'https://mr.wikipedia.org/wiki/%E0%A4%AA%E0%A4%BE%E0%A4%AF%E0%A4%A5%E0%A5%89%E0%A4%A8_(%E0%A4%86%E0%A4%9C%E0%A5%8D%E0%A4%9E%E0%A4%BE%E0%A4%B5%E0%A4%B2%E0%A5%80_%E0%A4%AD%E0%A4%BE%E0%A4%B7%E0%A4%BE)',\n",
       " 'https://nl.wikipedia.org/wiki/Python',\n",
       " 'https://ja.wikipedia.org/wiki/%E3%83%91%E3%82%A4%E3%82%BD%E3%83%B3',\n",
       " 'https://no.wikipedia.org/wiki/Pyton',\n",
       " 'https://pl.wikipedia.org/wiki/Pyton',\n",
       " 'https://pt.wikipedia.org/wiki/Python_(desambigua%C3%A7%C3%A3o)',\n",
       " 'https://ru.wikipedia.org/wiki/Python_(%D0%B7%D0%BD%D0%B0%D1%87%D0%B5%D0%BD%D0%B8%D1%8F)',\n",
       " 'https://sk.wikipedia.org/wiki/Python',\n",
       " 'https://sr.wikipedia.org/wiki/%D0%9F%D0%B8%D1%82%D0%BE%D0%BD_(%D0%B2%D0%B8%D1%88%D0%B5%D0%B7%D0%BD%D0%B0%D1%87%D0%BD%D0%B0_%D0%BE%D0%B4%D1%80%D0%B5%D0%B4%D0%BD%D0%B8%D1%86%D0%B0)',\n",
       " 'https://sh.wikipedia.org/wiki/Python',\n",
       " 'https://fi.wikipedia.org/wiki/Python',\n",
       " 'https://sv.wikipedia.org/wiki/Pyton',\n",
       " 'https://th.wikipedia.org/wiki/%E0%B9%84%E0%B8%9E%E0%B8%97%E0%B8%AD%E0%B8%99',\n",
       " 'https://tr.wikipedia.org/wiki/Python',\n",
       " 'https://uk.wikipedia.org/wiki/%D0%9F%D1%96%D1%84%D0%BE%D0%BD',\n",
       " 'https://ur.wikipedia.org/wiki/%D9%BE%D8%A7%D8%A6%DB%8C%D8%AA%DA%BE%D9%88%D9%86',\n",
       " 'https://vi.wikipedia.org/wiki/Python',\n",
       " 'https://zh.wikipedia.org/wiki/Python_(%E6%B6%88%E6%AD%A7%E4%B9%89)',\n",
       " 'https://www.wikidata.org/wiki/Special:EntityPage/Q747452#sitelinks-wikipedia',\n",
       " '//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License',\n",
       " '//creativecommons.org/licenses/by-sa/3.0/',\n",
       " '//foundation.wikimedia.org/wiki/Terms_of_Use',\n",
       " '//foundation.wikimedia.org/wiki/Privacy_policy',\n",
       " '//www.wikimediafoundation.org/',\n",
       " 'https://foundation.wikimedia.org/wiki/Privacy_policy',\n",
       " '/wiki/Wikipedia:About',\n",
       " '/wiki/Wikipedia:General_disclaimer',\n",
       " '//en.wikipedia.org/wiki/Wikipedia:Contact_us',\n",
       " 'https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute',\n",
       " 'https://stats.wikimedia.org/v2/#/en.wikipedia.org',\n",
       " 'https://foundation.wikimedia.org/wiki/Cookie_statement',\n",
       " '//en.m.wikipedia.org/w/index.php?title=Python&mobileaction=toggle_view_mobile',\n",
       " 'https://wikimediafoundation.org/',\n",
       " 'https://www.mediawiki.org/']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "def processLink(tag):\n",
    "    try:  \n",
    "        return tag.get('href')\n",
    "    except:\n",
    "        return None\n",
    "rests = [processLink(tag) for tag in soup.find_all('a', href=True)]\n",
    "rests = list(filter(lambda x: x, rests))\n",
    "rests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Titles that have changed in the United States Code since its last release point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'http://uscode.house.gov/download/download.shtml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(url)\n",
    "soup = BeautifulSoup(res.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "def processTitle(tag):\n",
    "    try:  \n",
    "        a = tag.text if tag.text.strip() != 'Appendix' else None\n",
    "        return a\n",
    "    except:\n",
    "        return None\n",
    "rests = [processTitle(tag) for tag in soup.select('div[id^=\"us\"]')]\n",
    "rests = list(filter(lambda x: x, rests))\n",
    "\n",
    "len(rests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rests = [tag.text if tag.text.strip() != 'Appendix' else None for tag in soup.select('div[id^=\"us\"]')]\n",
    "rests = list(filter(lambda x: x, rests))\n",
    "len(rests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Python list with the top ten FBI's Most Wanted names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.fbi.gov/wanted/topten'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(url)\n",
    "soup = BeautifulSoup(res.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['JASON DEREK BROWN',\n",
       " 'ALEXIS FLORES',\n",
       " 'EUGENE PALMER',\n",
       " 'SANTIAGO VILLALBA MEDEROS',\n",
       " 'RAFAEL CARO-QUINTERO',\n",
       " 'ROBERT WILLIAM FISHER',\n",
       " 'BHADRESHKUMAR CHETANBHAI PATEL',\n",
       " 'ARNOLDO JIMENEZ',\n",
       " 'ALEJANDRO ROSALES CASTILLO',\n",
       " 'YASER ABDEL SAID']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code \n",
    "def processFBI(tag):\n",
    "    try:  \n",
    "        return tag.text.strip()\n",
    "    except:\n",
    "        return None\n",
    "rests = [processFBI(tag) for tag in soup.select('h3')]\n",
    "rests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.emsc-csem.org/Earthquake/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(url)\n",
    "soup = BeautifulSoup(res.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>earthquake2020-01-30   19:47:40.621min ago</td>\n",
       "      <td>39.52  N</td>\n",
       "      <td>29.43 E</td>\n",
       "      <td>WESTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>earthquake2020-01-30   19:32:48.036min ago</td>\n",
       "      <td>2.46  N</td>\n",
       "      <td>98.99 E</td>\n",
       "      <td>NORTHERN SUMATRA, INDONESIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>earthquake2020-01-30   18:56:47.61hr 12min ago</td>\n",
       "      <td>38.99  N</td>\n",
       "      <td>27.87 E</td>\n",
       "      <td>WESTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>earthquake2020-01-30   18:43:40.81hr 25min ago</td>\n",
       "      <td>35.19  N</td>\n",
       "      <td>27.96 E</td>\n",
       "      <td>DODECANESE ISLANDS, GREECE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>earthquake2020-01-30   18:42:54.01hr 26min ago</td>\n",
       "      <td>19.20  N</td>\n",
       "      <td>155.45 W</td>\n",
       "      <td>ISLAND OF HAWAII, HAWAII</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>earthquake2020-01-30   18:40:03.81hr 29min ago</td>\n",
       "      <td>35.93  N</td>\n",
       "      <td>28.11 E</td>\n",
       "      <td>EASTERN MEDITERRANEAN SEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>earthquake2020-01-30   18:38:55.51hr 30min ago</td>\n",
       "      <td>39.01  N</td>\n",
       "      <td>27.86 E</td>\n",
       "      <td>WESTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>earthquake2020-01-30   18:36:42.51hr 32min ago</td>\n",
       "      <td>38.45  N</td>\n",
       "      <td>39.19 E</td>\n",
       "      <td>EASTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>earthquake2020-01-30   18:34:00.11hr 35min ago</td>\n",
       "      <td>49.97  N</td>\n",
       "      <td>177.83 W</td>\n",
       "      <td>SOUTH OF ALEUTIAN ISLANDS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>earthquake2020-01-30   18:28:47.11hr 40min ago</td>\n",
       "      <td>38.20  N</td>\n",
       "      <td>38.73 E</td>\n",
       "      <td>EASTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>earthquake2020-01-30   18:28:36.31hr 40min ago</td>\n",
       "      <td>39.03  N</td>\n",
       "      <td>27.85 E</td>\n",
       "      <td>WESTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>earthquake2020-01-30   18:26:20.91hr 43min ago</td>\n",
       "      <td>35.25  N</td>\n",
       "      <td>27.97 E</td>\n",
       "      <td>DODECANESE ISLANDS, GREECE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>earthquake2020-01-30   18:09:11.42hr 00min ago</td>\n",
       "      <td>41.79  N</td>\n",
       "      <td>20.12 E</td>\n",
       "      <td>ALBANIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>earthquake2020-01-30   17:58:07.92hr 11min ago</td>\n",
       "      <td>9.95  S</td>\n",
       "      <td>160.90 E</td>\n",
       "      <td>SOLOMON ISLANDS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>earthquake2020-01-30   17:57:57.12hr 11min ago</td>\n",
       "      <td>39.09  N</td>\n",
       "      <td>30.17 E</td>\n",
       "      <td>WESTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>earthquake2020-01-30   17:34:59.02hr 34min ago</td>\n",
       "      <td>38.40  N</td>\n",
       "      <td>15.36 E</td>\n",
       "      <td>SICILY, ITALY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>earthquake2020-01-30   17:29:59.72hr 39min ago</td>\n",
       "      <td>35.09  N</td>\n",
       "      <td>95.22 W</td>\n",
       "      <td>OKLAHOMA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>earthquake2020-01-30   17:29:14.52hr 40min ago</td>\n",
       "      <td>35.31  N</td>\n",
       "      <td>28.03 E</td>\n",
       "      <td>EASTERN MEDITERRANEAN SEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>earthquake2020-01-30   17:26:26.52hr 42min ago</td>\n",
       "      <td>19.45  N</td>\n",
       "      <td>155.19 W</td>\n",
       "      <td>ISLAND OF HAWAII, HAWAII</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>earthquake2020-01-30   17:21:08.12hr 48min ago</td>\n",
       "      <td>17.97  N</td>\n",
       "      <td>66.78 W</td>\n",
       "      <td>PUERTO RICO REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>earthquake2020-01-30   17:15:36.02hr 53min ago</td>\n",
       "      <td>17.96  N</td>\n",
       "      <td>66.78 W</td>\n",
       "      <td>PUERTO RICO REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>earthquake2020-01-30   17:10:27.52hr 58min ago</td>\n",
       "      <td>17.96  N</td>\n",
       "      <td>66.76 W</td>\n",
       "      <td>PUERTO RICO REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>earthquake2020-01-30   17:10:22.62hr 59min ago</td>\n",
       "      <td>20.01  N</td>\n",
       "      <td>155.62 W</td>\n",
       "      <td>HAWAII REGION, HAWAII</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>earthquake2020-01-30   17:08:59.03hr 00min ago</td>\n",
       "      <td>8.25  S</td>\n",
       "      <td>118.05 E</td>\n",
       "      <td>SUMBAWA REGION, INDONESIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>earthquake2020-01-30   17:07:34.03hr 01min ago</td>\n",
       "      <td>36.70  N</td>\n",
       "      <td>121.33 W</td>\n",
       "      <td>CENTRAL CALIFORNIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>earthquake2020-01-30   17:06:15.83hr 03min ago</td>\n",
       "      <td>17.98  N</td>\n",
       "      <td>66.86 W</td>\n",
       "      <td>PUERTO RICO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>earthquake2020-01-30   16:59:40.33hr 09min ago</td>\n",
       "      <td>19.39  N</td>\n",
       "      <td>155.28 W</td>\n",
       "      <td>ISLAND OF HAWAII, HAWAII</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>earthquake2020-01-30   16:48:05.93hr 21min ago</td>\n",
       "      <td>19.42  N</td>\n",
       "      <td>155.28 W</td>\n",
       "      <td>ISLAND OF HAWAII, HAWAII</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>earthquake2020-01-30   16:31:34.53hr 37min ago</td>\n",
       "      <td>69.15  N</td>\n",
       "      <td>147.06 W</td>\n",
       "      <td>NORTHERN ALASKA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>earthquake2020-01-30   16:24:22.53hr 45min ago</td>\n",
       "      <td>35.13  N</td>\n",
       "      <td>27.95 E</td>\n",
       "      <td>DODECANESE ISLANDS, GREECE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>earthquake2020-01-30   16:20:41.03hr 48min ago</td>\n",
       "      <td>35.38  N</td>\n",
       "      <td>28.05 E</td>\n",
       "      <td>EASTERN MEDITERRANEAN SEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>earthquake2020-01-30   16:07:04.64hr 02min ago</td>\n",
       "      <td>35.66  N</td>\n",
       "      <td>117.48 W</td>\n",
       "      <td>SOUTHERN CALIFORNIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>earthquake2020-01-30   16:03:06.94hr 06min ago</td>\n",
       "      <td>36.70  N</td>\n",
       "      <td>121.36 W</td>\n",
       "      <td>CENTRAL CALIFORNIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>earthquake2020-01-30   15:58:49.64hr 10min ago</td>\n",
       "      <td>31.13  N</td>\n",
       "      <td>103.25 W</td>\n",
       "      <td>WESTERN TEXAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>earthquake2020-01-30   15:53:50.14hr 15min ago</td>\n",
       "      <td>35.28  N</td>\n",
       "      <td>28.09 E</td>\n",
       "      <td>EASTERN MEDITERRANEAN SEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>earthquake2020-01-30   15:47:59.54hr 21min ago</td>\n",
       "      <td>19.49  N</td>\n",
       "      <td>155.24 W</td>\n",
       "      <td>ISLAND OF HAWAII, HAWAII</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>earthquake2020-01-30   15:43:23.74hr 26min ago</td>\n",
       "      <td>40.35  N</td>\n",
       "      <td>27.13 E</td>\n",
       "      <td>WESTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>earthquake2020-01-30   15:37:07.04hr 32min ago</td>\n",
       "      <td>0.41  S</td>\n",
       "      <td>124.13 E</td>\n",
       "      <td>MOLUCCA SEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>earthquake2020-01-30   15:31:32.74hr 37min ago</td>\n",
       "      <td>19.12  N</td>\n",
       "      <td>155.42 W</td>\n",
       "      <td>ISLAND OF HAWAII, HAWAII</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>earthquake2020-01-30   15:31:31.54hr 37min ago</td>\n",
       "      <td>39.24  N</td>\n",
       "      <td>18.72 E</td>\n",
       "      <td>SOUTHERN ITALY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>earthquake2020-01-30   15:30:47.44hr 38min ago</td>\n",
       "      <td>39.06  N</td>\n",
       "      <td>27.86 E</td>\n",
       "      <td>WESTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>earthquake2020-01-30   15:28:46.24hr 40min ago</td>\n",
       "      <td>17.90  N</td>\n",
       "      <td>66.63 W</td>\n",
       "      <td>PUERTO RICO REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>earthquake2020-01-30   15:27:18.04hr 42min ago</td>\n",
       "      <td>49.41  N</td>\n",
       "      <td>0.01 W</td>\n",
       "      <td>FRANCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>earthquake2020-01-30   15:24:22.04hr 45min ago</td>\n",
       "      <td>16.85  N</td>\n",
       "      <td>100.10 W</td>\n",
       "      <td>OFFSHORE GUERRERO, MEXICO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>earthquake2020-01-30   15:12:32.64hr 56min ago</td>\n",
       "      <td>35.28  N</td>\n",
       "      <td>28.06 E</td>\n",
       "      <td>EASTERN MEDITERRANEAN SEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>earthquake2020-01-30   15:02:42.45hr 06min ago</td>\n",
       "      <td>39.08  N</td>\n",
       "      <td>27.85 E</td>\n",
       "      <td>WESTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>earthquake2020-01-30   15:00:42.95hr 08min ago</td>\n",
       "      <td>56.45  N</td>\n",
       "      <td>150.00 W</td>\n",
       "      <td>GULF OF ALASKA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>earthquake2020-01-30   14:48:15.75hr 21min ago</td>\n",
       "      <td>39.09  N</td>\n",
       "      <td>27.83 E</td>\n",
       "      <td>WESTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>earthquake2020-01-30   14:43:53.45hr 25min ago</td>\n",
       "      <td>36.70  N</td>\n",
       "      <td>121.34 W</td>\n",
       "      <td>CENTRAL CALIFORNIA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              date         lat        long  \\\n",
       "0       earthquake2020-01-30   19:47:40.621min ago  39.52  N     29.43 E     \n",
       "1       earthquake2020-01-30   19:32:48.036min ago   2.46  N     98.99 E     \n",
       "2   earthquake2020-01-30   18:56:47.61hr 12min ago  38.99  N     27.87 E     \n",
       "3   earthquake2020-01-30   18:43:40.81hr 25min ago  35.19  N     27.96 E     \n",
       "4   earthquake2020-01-30   18:42:54.01hr 26min ago  19.20  N    155.45 W     \n",
       "5   earthquake2020-01-30   18:40:03.81hr 29min ago  35.93  N     28.11 E     \n",
       "6   earthquake2020-01-30   18:38:55.51hr 30min ago  39.01  N     27.86 E     \n",
       "7   earthquake2020-01-30   18:36:42.51hr 32min ago  38.45  N     39.19 E     \n",
       "8   earthquake2020-01-30   18:34:00.11hr 35min ago  49.97  N    177.83 W     \n",
       "9   earthquake2020-01-30   18:28:47.11hr 40min ago  38.20  N     38.73 E     \n",
       "10  earthquake2020-01-30   18:28:36.31hr 40min ago  39.03  N     27.85 E     \n",
       "11  earthquake2020-01-30   18:26:20.91hr 43min ago  35.25  N     27.97 E     \n",
       "12  earthquake2020-01-30   18:09:11.42hr 00min ago  41.79  N     20.12 E     \n",
       "13  earthquake2020-01-30   17:58:07.92hr 11min ago   9.95  S    160.90 E     \n",
       "14  earthquake2020-01-30   17:57:57.12hr 11min ago  39.09  N     30.17 E     \n",
       "15  earthquake2020-01-30   17:34:59.02hr 34min ago  38.40  N     15.36 E     \n",
       "16  earthquake2020-01-30   17:29:59.72hr 39min ago  35.09  N     95.22 W     \n",
       "17  earthquake2020-01-30   17:29:14.52hr 40min ago  35.31  N     28.03 E     \n",
       "18  earthquake2020-01-30   17:26:26.52hr 42min ago  19.45  N    155.19 W     \n",
       "19  earthquake2020-01-30   17:21:08.12hr 48min ago  17.97  N     66.78 W     \n",
       "20  earthquake2020-01-30   17:15:36.02hr 53min ago  17.96  N     66.78 W     \n",
       "21  earthquake2020-01-30   17:10:27.52hr 58min ago  17.96  N     66.76 W     \n",
       "22  earthquake2020-01-30   17:10:22.62hr 59min ago  20.01  N    155.62 W     \n",
       "23  earthquake2020-01-30   17:08:59.03hr 00min ago   8.25  S    118.05 E     \n",
       "24  earthquake2020-01-30   17:07:34.03hr 01min ago  36.70  N    121.33 W     \n",
       "25  earthquake2020-01-30   17:06:15.83hr 03min ago  17.98  N     66.86 W     \n",
       "26  earthquake2020-01-30   16:59:40.33hr 09min ago  19.39  N    155.28 W     \n",
       "27  earthquake2020-01-30   16:48:05.93hr 21min ago  19.42  N    155.28 W     \n",
       "28  earthquake2020-01-30   16:31:34.53hr 37min ago  69.15  N    147.06 W     \n",
       "29  earthquake2020-01-30   16:24:22.53hr 45min ago  35.13  N     27.95 E     \n",
       "30  earthquake2020-01-30   16:20:41.03hr 48min ago  35.38  N     28.05 E     \n",
       "31  earthquake2020-01-30   16:07:04.64hr 02min ago  35.66  N    117.48 W     \n",
       "32  earthquake2020-01-30   16:03:06.94hr 06min ago  36.70  N    121.36 W     \n",
       "33  earthquake2020-01-30   15:58:49.64hr 10min ago  31.13  N    103.25 W     \n",
       "34  earthquake2020-01-30   15:53:50.14hr 15min ago  35.28  N     28.09 E     \n",
       "35  earthquake2020-01-30   15:47:59.54hr 21min ago  19.49  N    155.24 W     \n",
       "36  earthquake2020-01-30   15:43:23.74hr 26min ago  40.35  N     27.13 E     \n",
       "37  earthquake2020-01-30   15:37:07.04hr 32min ago   0.41  S    124.13 E     \n",
       "38  earthquake2020-01-30   15:31:32.74hr 37min ago  19.12  N    155.42 W     \n",
       "39  earthquake2020-01-30   15:31:31.54hr 37min ago  39.24  N     18.72 E     \n",
       "40  earthquake2020-01-30   15:30:47.44hr 38min ago  39.06  N     27.86 E     \n",
       "41  earthquake2020-01-30   15:28:46.24hr 40min ago  17.90  N     66.63 W     \n",
       "42  earthquake2020-01-30   15:27:18.04hr 42min ago  49.41  N      0.01 W     \n",
       "43  earthquake2020-01-30   15:24:22.04hr 45min ago  16.85  N    100.10 W     \n",
       "44  earthquake2020-01-30   15:12:32.64hr 56min ago  35.28  N     28.06 E     \n",
       "45  earthquake2020-01-30   15:02:42.45hr 06min ago  39.08  N     27.85 E     \n",
       "46  earthquake2020-01-30   15:00:42.95hr 08min ago  56.45  N    150.00 W     \n",
       "47  earthquake2020-01-30   14:48:15.75hr 21min ago  39.09  N     27.83 E     \n",
       "48  earthquake2020-01-30   14:43:53.45hr 25min ago  36.70  N    121.34 W     \n",
       "\n",
       "                          region  \n",
       "0                 WESTERN TURKEY  \n",
       "1    NORTHERN SUMATRA, INDONESIA  \n",
       "2                 WESTERN TURKEY  \n",
       "3     DODECANESE ISLANDS, GREECE  \n",
       "4       ISLAND OF HAWAII, HAWAII  \n",
       "5      EASTERN MEDITERRANEAN SEA  \n",
       "6                 WESTERN TURKEY  \n",
       "7                 EASTERN TURKEY  \n",
       "8      SOUTH OF ALEUTIAN ISLANDS  \n",
       "9                 EASTERN TURKEY  \n",
       "10                WESTERN TURKEY  \n",
       "11    DODECANESE ISLANDS, GREECE  \n",
       "12                       ALBANIA  \n",
       "13               SOLOMON ISLANDS  \n",
       "14                WESTERN TURKEY  \n",
       "15                 SICILY, ITALY  \n",
       "16                      OKLAHOMA  \n",
       "17     EASTERN MEDITERRANEAN SEA  \n",
       "18      ISLAND OF HAWAII, HAWAII  \n",
       "19            PUERTO RICO REGION  \n",
       "20            PUERTO RICO REGION  \n",
       "21            PUERTO RICO REGION  \n",
       "22         HAWAII REGION, HAWAII  \n",
       "23     SUMBAWA REGION, INDONESIA  \n",
       "24            CENTRAL CALIFORNIA  \n",
       "25                   PUERTO RICO  \n",
       "26      ISLAND OF HAWAII, HAWAII  \n",
       "27      ISLAND OF HAWAII, HAWAII  \n",
       "28               NORTHERN ALASKA  \n",
       "29    DODECANESE ISLANDS, GREECE  \n",
       "30     EASTERN MEDITERRANEAN SEA  \n",
       "31           SOUTHERN CALIFORNIA  \n",
       "32            CENTRAL CALIFORNIA  \n",
       "33                 WESTERN TEXAS  \n",
       "34     EASTERN MEDITERRANEAN SEA  \n",
       "35      ISLAND OF HAWAII, HAWAII  \n",
       "36                WESTERN TURKEY  \n",
       "37                   MOLUCCA SEA  \n",
       "38      ISLAND OF HAWAII, HAWAII  \n",
       "39                SOUTHERN ITALY  \n",
       "40                WESTERN TURKEY  \n",
       "41            PUERTO RICO REGION  \n",
       "42                        FRANCE  \n",
       "43     OFFSHORE GUERRERO, MEXICO  \n",
       "44     EASTERN MEDITERRANEAN SEA  \n",
       "45                WESTERN TURKEY  \n",
       "46                GULF OF ALASKA  \n",
       "47                WESTERN TURKEY  \n",
       "48            CENTRAL CALIFORNIA  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "def processEQ(tag):\n",
    "    m = tag.find_all(\"td\")\n",
    "    try:\n",
    "        return { \n",
    "        'date': m[3].text.strip(),\n",
    "        'lat': m[4].text + ' ' + m[5].text,\n",
    "        'long': m[6].text.strip() + ' ' + m[7].text,\n",
    "        'region': m[11].text\n",
    "    }  \n",
    "       \n",
    "    except:\n",
    "        return None\n",
    "rests = soup.find_all('tbody')[0]    \n",
    "rests_dic = [processEQ(tag) for tag in rests.find_all('tr')[1:]]\n",
    "Dataframe = pd.DataFrame(rests_dic)\n",
    "Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the date, days, title, city, country of next 25 hackathon events as a Pandas dataframe table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://hackevents.co/hackathons'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count number of tweets by a given Twitter account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to include a ***try/except block*** for account names not found. \n",
    "<br>***Hint:*** the program should count the number of tweets for any provided account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/Pontifex_es'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(url)\n",
    "soup = BeautifulSoup(res.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2.307 Tweets']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "def processTwitt(tag):\n",
    "    try:    \n",
    "        return tag.attrs['title']\n",
    "    except:\n",
    "        return None\n",
    "rests = [processTwitt(tag) for tag in soup.find_all('a', class_=\"ProfileNav-stat ProfileNav-stat--link u-borderUserColor u-textCenter js-tooltip js-nav\")]\n",
    "\n",
    "rests = list(filter(lambda x: x, rests))\n",
    "rests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of followers of a given twitter account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to include a ***try/except block*** in case account/s name not found. \n",
    "<br>***Hint:*** the program should count the followers for any provided account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/Pontifex_es'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(url)\n",
    "soup = BeautifulSoup(res.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'17.349.862 Seguidores'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "def processTwitt(tag):\n",
    "    try:    \n",
    "        return tag.attrs['title']\n",
    "    except:\n",
    "        return None\n",
    "rests = [processTwitt(tag) for tag in soup.find_all('a', class_='ProfileNav-stat ProfileNav-stat--link u-borderUserColor u-textCenter js-tooltip js-openSignupDialog js-nonNavigable u-textUserColor')]\n",
    "\n",
    "rests = list(filter(lambda x: x, rests))\n",
    "rests[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List all language names and number of related articles in the order they appear in wikipedia.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.wikipedia.org/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(url)\n",
    "soup = BeautifulSoup(res.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['English',\n",
       " 'æ\\x97¥æ\\x9c¬èª\\x9e',\n",
       " 'Deutsch',\n",
       " 'EspaÃ±ol',\n",
       " 'Ð\\xa0Ñ\\x83Ñ\\x81Ñ\\x81ÐºÐ¸Ð¹',\n",
       " 'FranÃ§ais',\n",
       " 'Italiano',\n",
       " 'ä¸\\xadæ\\x96\\x87',\n",
       " 'PortuguÃªs',\n",
       " 'Polski']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "def processWiki(tag):\n",
    "    try:    \n",
    "        return tag.find('strong').text\n",
    "    except:\n",
    "        return None\n",
    "rests = [processWiki(tag) for tag in soup.select('.link-box')]\n",
    "rests\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5\\xa0994\\xa0000+',\n",
       " '1\\xa0185\\xa0000+',\n",
       " '2\\xa0385\\xa0000+',\n",
       " '1\\xa0571\\xa0000+',\n",
       " '1\\xa0590\\xa0000+',\n",
       " '2\\xa0171\\xa0000+',\n",
       " '1\\xa0576\\xa0000+',\n",
       " '1\\xa0090\\xa0000+',\n",
       " '1\\xa0018\\xa0000+',\n",
       " '1\\xa0379\\xa0000+']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def processWiki2(tag):\n",
    "    try:    \n",
    "        return tag.find('bdi').text\n",
    "    except:\n",
    "        return None\n",
    "rests2 = [processWiki2(tag) for tag in soup.select('.link-box')]\n",
    "rests2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A list with the different kind of datasets available in data.gov.uk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://data.gov.uk/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 languages by number of native speakers stored in a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape a certain number of tweets of a given Twitter account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMDB's Top 250 data (movie name, Initial release, director name and stars) as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url = 'https://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Movie name, year and a brief summary of the top 10 random movies (IMDB) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the url you will scrape in this exercise\n",
    "url = 'http://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the live weather report (temperature, wind speed, description and weather) of a given city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://openweathermap.org/current\n",
    "city = city=input('Enter the city:')\n",
    "url = 'http://api.openweathermap.org/data/2.5/weather?'+'q='+city+'&APPID=b35975e18dc93725acb092f7272cc6b8&units=metric'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Book name,price and stock availability as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise. \n",
    "# It is a fictional bookstore created to be scraped. \n",
    "url = 'http://books.toscrape.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
