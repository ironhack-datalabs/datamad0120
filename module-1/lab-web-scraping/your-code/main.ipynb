{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Lab\n",
    "\n",
    "You will find in this notebook some scrapy exercises to practise your scraping skills.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "- Check the response status code for each request to ensure you have obtained the intended contennt.\n",
    "- Print the response text in each request to understand the kind of info you are getting and its format.\n",
    "- Check for patterns in the response text to extract the data/info requested in each question.\n",
    "- Visit each url and take a look at its source through Chrome DevTools. You'll need to identify the html tags, special class names etc. used for the html content you are expected to extract."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide) documentation \n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [lxml lib](https://lxml.de/)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are the libraries and modules you may need. `requests`,  `BeautifulSoup` and `pandas` are imported for you. If you prefer to use additional libraries feel free to uncomment them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "# from pprint import pprint\n",
    "# from lxml import html\n",
    "# from lxml.html import fromstring\n",
    "# import urllib.request\n",
    "# from urllib.request import urlopen\n",
    "# import random\n",
    "# import re\n",
    "# import scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "data = requests.get(url).text\n",
    "soup = BeautifulSoup(data, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the names of the trending developers retrieved in the previous step.\n",
    "\n",
    "Your output should be a Python list of developer names. Each name should not contain any html tag.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Find out the html tag and class names used for the developer names. You can achieve this using Chrome DevTools.\n",
    "\n",
    "1. Use BeautifulSoup to extract all the html elements that contain the developer names.\n",
    "\n",
    "1. Use string manipulation techniques to replace whitespaces and linebreaks (i.e. `\\n`) in the *text* of each html element. Use a list to store the clean names.\n",
    "\n",
    "1. Print the list of names.\n",
    "\n",
    "Your output should look like below:\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (神楽坂覚々)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mattn mattn',\n",
       " 'emmet-vim amueller',\n",
       " 'Andreas Mueller brianc',\n",
       " 'word_cloud ErikSchierboom',\n",
       " 'Brian C JohnSundell',\n",
       " 'node-postgres siddharthkp',\n",
       " 'Erik Schierboom jasonwilliams',\n",
       " 'knockout-pre-rendered kyleconroy',\n",
       " 'John Sundell asottile',\n",
       " 'SwiftTips wcandillon',\n",
       " 'Siddharth Kshetrapal cmyr',\n",
       " 'bundlesize panva',\n",
       " 'Jason Williams amilajack',\n",
       " 'boa KristofferC',\n",
       " 'Kyle Conroy gcanti',\n",
       " 'sqlc greatghoul',\n",
       " 'Anthony Sottile miekg',\n",
       " 'pyupgrade Naturalclar',\n",
       " 'William Candillon lucasmichot',\n",
       " 'can-it-be-done-in-react-native ageitgey',\n",
       " 'Colin Rofls phil-opp',\n",
       " 'RustPlayground tony',\n",
       " 'Filip Skokan isaacs',\n",
       " 'node-oidc-provider 4ian',\n",
       " 'Amila Welihinda satya164']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def developers():\n",
    "    names = soup.select('h1 a[href]')\n",
    "    usernames = soup.select('p a.link-gray')\n",
    "    combined = zip(names,usernames)\n",
    "    return [f'{developer.text.strip()} {nick.text.strip()}' for (developer,nick) in combined]\n",
    "developers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the trending Python repositories in GitHub\n",
    "\n",
    "The steps to solve this problem is similar to the previous one except that you need to find out the repository names instead of developer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/python?since=daily'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BlankerL /\\n\\n\\n\\n      DXY-2019-nCoV-Crawler',\n",
       " 'gto76 /\\n\\n\\n\\n      python-cheatsheet',\n",
       " 'smicallef /\\n\\n\\n\\n      spiderfoot',\n",
       " 'sebastianruder /\\n\\n\\n\\n      NLP-progress',\n",
       " 'rish-16 /\\n\\n\\n\\n      sight',\n",
       " 'luong-komorebi /\\n\\n\\n\\n      Awesome-Linux-Software',\n",
       " 'vinta /\\n\\n\\n\\n      awesome-python',\n",
       " 'nicrusso7 /\\n\\n\\n\\n      rex-gym',\n",
       " 'baowenbo /\\n\\n\\n\\n      DAIN',\n",
       " 'eastlakeside /\\n\\n\\n\\n      interpy-zh',\n",
       " 'tiangolo /\\n\\n\\n\\n      fastapi',\n",
       " 'explosion /\\n\\n\\n\\n      thinc',\n",
       " 'willmcgugan /\\n\\n\\n\\n      rich',\n",
       " 'MatthewPierson /\\n\\n\\n\\n      Vieux',\n",
       " 'CCExtractor /\\n\\n\\n\\n      vardbg',\n",
       " 'dragen1860 /\\n\\n\\n\\n      Deep-Learning-with-TensorFlow-book',\n",
       " 'brendangregg /\\n\\n\\n\\n      bpf-perf-tools-book',\n",
       " 'googleapis /\\n\\n\\n\\n      google-cloud-python',\n",
       " 'anchore /\\n\\n\\n\\n      anchore-engine',\n",
       " 'aj-4 /\\n\\n\\n\\n      tinder-swipe-bot',\n",
       " 'pytorch /\\n\\n\\n\\n      captum',\n",
       " 'samuelhwilliams /\\n\\n\\n\\n      Eel',\n",
       " 'salesforce /\\n\\n\\n\\n      policy_sentry',\n",
       " 'yzhao062 /\\n\\n\\n\\n      anomaly-detection-resources',\n",
       " 'vineetjohn /\\n\\n\\n\\n      daily-coding-problem']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "res = requests.get('https://github.com/trending/python?since=daily').text\n",
    "res_soup = BeautifulSoup(res, 'html.parser')\n",
    "def trending_repos():\n",
    "    repos = res_soup.select('h1 a[href]')\n",
    "    #repos = res_soup.select('h1 a[href]')\n",
    "    return [repo.text.strip() for repo in repos]\n",
    "trending_repos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display all the image links from Walt Disney wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/Walt_Disney'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['en.wikipedia.org/wiki/File:Walt_Disney_1946.JPG',\n",
       " 'en.wikipedia.org/wiki/File:Walt_Disney_1942_signature.svg',\n",
       " 'en.wikipedia.org/wiki/File:Walt_Disney_envelope_ca._1921.jpg',\n",
       " 'en.wikipedia.org/wiki/File:Trolley_Troubles_poster.jpg',\n",
       " 'en.wikipedia.org/wiki/File:Walt_Disney_and_his_cartoon_creation_%22Mickey_Mouse%22_-_National_Board_of_Review_Magazine.jpg',\n",
       " 'en.wikipedia.org/wiki/File:Steamboat-willie.jpg',\n",
       " 'en.wikipedia.org/wiki/File:Walt_Disney_1935.jpg',\n",
       " 'en.wikipedia.org/wiki/File:Walt_Disney_Snow_white_1937_trailer_screenshot_(13).jpg',\n",
       " 'en.wikipedia.org/wiki/File:Disney_drawing_goofy.jpg',\n",
       " 'en.wikipedia.org/wiki/File:DisneySchiphol1951.jpg',\n",
       " 'en.wikipedia.org/wiki/File:WaltDisneyplansDisneylandDec1954.jpg',\n",
       " 'en.wikipedia.org/wiki/File:Walt_disney_portrait_right.jpg',\n",
       " 'en.wikipedia.org/wiki/File:Walt_Disney_Grave.JPG',\n",
       " 'en.wikipedia.org/wiki/File:Roy_O._Disney_with_Company_at_Press_Conference.jpg',\n",
       " 'en.wikipedia.org/wiki/File:Disney_Display_Case.JPG',\n",
       " 'en.wikipedia.org/wiki/File:Disney1968.jpg',\n",
       " 'en.wikipedia.org/wiki/File:The_Walt_Disney_Company_Logo.svg',\n",
       " 'en.wikipedia.org/wiki/File:Animation_disc.svg',\n",
       " 'en.wikipedia.org/wiki/File:P_vip.svg',\n",
       " 'en.wikipedia.org/wiki/File:Magic_Kingdom_castle.jpg',\n",
       " 'en.wikipedia.org/wiki/File:Video-x-generic.svg',\n",
       " 'en.wikipedia.org/wiki/File:Flag_of_Los_Angeles_County,_California.svg',\n",
       " 'en.wikipedia.org/wiki/File:Blank_television_set.svg',\n",
       " 'en.wikipedia.org/wiki/File:Flag_of_the_United_States.svg']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "links_raw = requests.get('https://en.wikipedia.org/wiki/Walt_Disney').text\n",
    "pretty_links = BeautifulSoup(links_raw, 'html.parser')\n",
    "def images():\n",
    "    return [f'en.wikipedia.org{image[\"href\"]}' for image in pretty_links.select('a.image')]\n",
    "    #return  [image['src'] for image in pretty_links.select('img')]\n",
    "images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve an arbitary Wikipedia page of \"Python\" and create a list of links on that page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://en.wikipedia.org/wiki/Python' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/w/load.php?lang=en&modules=ext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediaBadges%7Cmediawiki.legacy.commonPrint%2Cshared%7Cmediawiki.skinning.interface%7Cmediawiki.toc.styles%7Cskins.vector.styles%7Cwikibase.client.init&only=styles&skin=vector',\n",
       " '/w/load.php?lang=en&modules=site.styles&only=styles&skin=vector',\n",
       " 'android-app://org.wikipedia/http/en.m.wikipedia.org/wiki/Python',\n",
       " '/w/index.php?title=Python&action=edit',\n",
       " '/w/index.php?title=Python&action=edit',\n",
       " '/static/apple-touch/wikipedia.png',\n",
       " '/static/favicon/wikipedia.ico',\n",
       " '/w/opensearch_desc.php',\n",
       " '//en.wikipedia.org/w/api.php?action=rsd',\n",
       " '//creativecommons.org/licenses/by-sa/3.0/',\n",
       " 'https://en.wikipedia.org/wiki/Python',\n",
       " '//login.wikimedia.org',\n",
       " '//meta.wikimedia.org',\n",
       " '#mw-head',\n",
       " '#p-search',\n",
       " 'https://en.wiktionary.org/wiki/Python',\n",
       " 'https://en.wiktionary.org/wiki/python',\n",
       " '#Snakes',\n",
       " '#Ancient_Greece',\n",
       " '#Media_and_entertainment',\n",
       " '#Computing',\n",
       " '#Engineering',\n",
       " '#Roller_coasters',\n",
       " '#Vehicles',\n",
       " '#Weaponry',\n",
       " '#People',\n",
       " '#Other_uses',\n",
       " '#See_also',\n",
       " '/w/index.php?title=Python&action=edit&section=1',\n",
       " '/wiki/Pythonidae',\n",
       " '/wiki/Python_(genus)',\n",
       " '/w/index.php?title=Python&action=edit&section=2',\n",
       " '/wiki/Python_(mythology)',\n",
       " '/wiki/Python_of_Aenus',\n",
       " '/wiki/Python_(painter)',\n",
       " '/wiki/Python_of_Byzantium',\n",
       " '/wiki/Python_of_Catana',\n",
       " '/w/index.php?title=Python&action=edit&section=3',\n",
       " '/wiki/Python_(film)',\n",
       " '/wiki/Pythons_2',\n",
       " '/wiki/Monty_Python',\n",
       " '/wiki/Python_(Monty)_Pictures',\n",
       " '/w/index.php?title=Python&action=edit&section=4',\n",
       " '/wiki/Python_(programming_language)',\n",
       " '/wiki/CPython',\n",
       " '/wiki/CMU_Common_Lisp',\n",
       " '/wiki/PERQ#PERQ_3',\n",
       " '/w/index.php?title=Python&action=edit&section=5',\n",
       " '/w/index.php?title=Python&action=edit&section=6',\n",
       " '/wiki/Python_(Busch_Gardens_Tampa_Bay)',\n",
       " '/wiki/Python_(Coney_Island,_Cincinnati,_Ohio)',\n",
       " '/wiki/Python_(Efteling)',\n",
       " '/w/index.php?title=Python&action=edit&section=7',\n",
       " '/wiki/Python_(automobile_maker)',\n",
       " '/wiki/Python_(Ford_prototype)',\n",
       " '/w/index.php?title=Python&action=edit&section=8',\n",
       " '/wiki/Colt_Python',\n",
       " '/wiki/Python_(missile)',\n",
       " '/wiki/Python_(nuclear_primary)',\n",
       " '/w/index.php?title=Python&action=edit&section=9',\n",
       " '/wiki/Python_Anghelo',\n",
       " '/w/index.php?title=Python&action=edit&section=10',\n",
       " '/wiki/PYTHON',\n",
       " '/w/index.php?title=Python&action=edit&section=11',\n",
       " '/wiki/Cython',\n",
       " '/wiki/Pyton',\n",
       " '/wiki/File:Disambig_gray.svg',\n",
       " '/wiki/Help:Disambiguation',\n",
       " 'https://en.wikipedia.org/w/index.php?title=Special:WhatLinksHere/Python&namespace=0',\n",
       " 'https://en.wikipedia.org/w/index.php?title=Python&oldid=937840393',\n",
       " '/wiki/Help:Category',\n",
       " '/wiki/Category:Disambiguation_pages',\n",
       " '/wiki/Category:Disambiguation_pages_with_short_description',\n",
       " '/wiki/Category:All_article_disambiguation_pages',\n",
       " '/wiki/Category:All_disambiguation_pages',\n",
       " '/wiki/Category:Animal_common_name_disambiguation_pages',\n",
       " '/wiki/Special:MyTalk',\n",
       " '/wiki/Special:MyContributions',\n",
       " '/w/index.php?title=Special:CreateAccount&returnto=Python',\n",
       " '/w/index.php?title=Special:UserLogin&returnto=Python',\n",
       " '/wiki/Python',\n",
       " '/wiki/Talk:Python',\n",
       " '/wiki/Python',\n",
       " '/w/index.php?title=Python&action=edit',\n",
       " '/w/index.php?title=Python&action=history',\n",
       " '/wiki/Main_Page',\n",
       " '/wiki/Main_Page',\n",
       " '/wiki/Wikipedia:Contents',\n",
       " '/wiki/Wikipedia:Featured_content',\n",
       " '/wiki/Portal:Current_events',\n",
       " '/wiki/Special:Random',\n",
       " 'https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&utm_medium=sidebar&utm_campaign=C13_en.wikipedia.org&uselang=en',\n",
       " '//shop.wikimedia.org',\n",
       " '/wiki/Help:Contents',\n",
       " '/wiki/Wikipedia:About',\n",
       " '/wiki/Wikipedia:Community_portal',\n",
       " '/wiki/Special:RecentChanges',\n",
       " '//en.wikipedia.org/wiki/Wikipedia:Contact_us',\n",
       " '/wiki/Special:WhatLinksHere/Python',\n",
       " '/wiki/Special:RecentChangesLinked/Python',\n",
       " '/wiki/Wikipedia:File_Upload_Wizard',\n",
       " '/wiki/Special:SpecialPages',\n",
       " '/w/index.php?title=Python&oldid=937840393',\n",
       " '/w/index.php?title=Python&action=info',\n",
       " 'https://www.wikidata.org/wiki/Special:EntityPage/Q747452',\n",
       " '/w/index.php?title=Special:CiteThisPage&page=Python&id=937840393',\n",
       " 'https://commons.wikimedia.org/wiki/Category:Python',\n",
       " '/w/index.php?title=Special:Book&bookcmd=book_creator&referer=Python',\n",
       " '/w/index.php?title=Special:ElectronPdf&page=Python&action=show-download-screen',\n",
       " '/w/index.php?title=Python&printable=yes',\n",
       " 'https://af.wikipedia.org/wiki/Python',\n",
       " 'https://als.wikipedia.org/wiki/Python',\n",
       " 'https://az.wikipedia.org/wiki/Python',\n",
       " 'https://bn.wikipedia.org/wiki/%E0%A6%AA%E0%A6%BE%E0%A6%87%E0%A6%A5%E0%A6%A8_(%E0%A6%A6%E0%A7%8D%E0%A6%AC%E0%A7%8D%E0%A6%AF%E0%A6%B0%E0%A7%8D%E0%A6%A5%E0%A6%A4%E0%A6%BE_%E0%A6%A8%E0%A6%BF%E0%A6%B0%E0%A6%B8%E0%A6%A8)',\n",
       " 'https://be.wikipedia.org/wiki/Python',\n",
       " 'https://bg.wikipedia.org/wiki/%D0%9F%D0%B8%D1%82%D0%BE%D0%BD_(%D0%BF%D0%BE%D1%8F%D1%81%D0%BD%D0%B5%D0%BD%D0%B8%D0%B5)',\n",
       " 'https://cs.wikipedia.org/wiki/Python_(rozcestn%C3%ADk)',\n",
       " 'https://da.wikipedia.org/wiki/Python',\n",
       " 'https://de.wikipedia.org/wiki/Python',\n",
       " 'https://eo.wikipedia.org/wiki/Pitono_(apartigilo)',\n",
       " 'https://eu.wikipedia.org/wiki/Python_(argipena)',\n",
       " 'https://fa.wikipedia.org/wiki/%D9%BE%D8%A7%DB%8C%D8%AA%D9%88%D9%86',\n",
       " 'https://fr.wikipedia.org/wiki/Python',\n",
       " 'https://ko.wikipedia.org/wiki/%ED%8C%8C%EC%9D%B4%EC%84%A0',\n",
       " 'https://hr.wikipedia.org/wiki/Python_(razdvojba)',\n",
       " 'https://io.wikipedia.org/wiki/Pitono',\n",
       " 'https://id.wikipedia.org/wiki/Python',\n",
       " 'https://ia.wikipedia.org/wiki/Python_(disambiguation)',\n",
       " 'https://is.wikipedia.org/wiki/Python_(a%C3%B0greining)',\n",
       " 'https://it.wikipedia.org/wiki/Python_(disambigua)',\n",
       " 'https://he.wikipedia.org/wiki/%D7%A4%D7%99%D7%AA%D7%95%D7%9F',\n",
       " 'https://ka.wikipedia.org/wiki/%E1%83%9E%E1%83%98%E1%83%97%E1%83%9D%E1%83%9C%E1%83%98_(%E1%83%9B%E1%83%A0%E1%83%90%E1%83%95%E1%83%90%E1%83%9A%E1%83%9B%E1%83%9C%E1%83%98%E1%83%A8%E1%83%95%E1%83%9C%E1%83%94%E1%83%9A%E1%83%9D%E1%83%95%E1%83%90%E1%83%9C%E1%83%98)',\n",
       " 'https://kg.wikipedia.org/wiki/Mboma_(nyoka)',\n",
       " 'https://la.wikipedia.org/wiki/Python_(discretiva)',\n",
       " 'https://lb.wikipedia.org/wiki/Python',\n",
       " 'https://hu.wikipedia.org/wiki/Python_(egy%C3%A9rtelm%C5%B1s%C3%ADt%C5%91_lap)',\n",
       " 'https://mr.wikipedia.org/wiki/%E0%A4%AA%E0%A4%BE%E0%A4%AF%E0%A4%A5%E0%A5%89%E0%A4%A8_(%E0%A4%86%E0%A4%9C%E0%A5%8D%E0%A4%9E%E0%A4%BE%E0%A4%B5%E0%A4%B2%E0%A5%80_%E0%A4%AD%E0%A4%BE%E0%A4%B7%E0%A4%BE)',\n",
       " 'https://nl.wikipedia.org/wiki/Python',\n",
       " 'https://ja.wikipedia.org/wiki/%E3%83%91%E3%82%A4%E3%82%BD%E3%83%B3',\n",
       " 'https://no.wikipedia.org/wiki/Pyton',\n",
       " 'https://pl.wikipedia.org/wiki/Pyton',\n",
       " 'https://pt.wikipedia.org/wiki/Python_(desambigua%C3%A7%C3%A3o)',\n",
       " 'https://ru.wikipedia.org/wiki/Python_(%D0%B7%D0%BD%D0%B0%D1%87%D0%B5%D0%BD%D0%B8%D1%8F)',\n",
       " 'https://sk.wikipedia.org/wiki/Python',\n",
       " 'https://sr.wikipedia.org/wiki/%D0%9F%D0%B8%D1%82%D0%BE%D0%BD_(%D0%B2%D0%B8%D1%88%D0%B5%D0%B7%D0%BD%D0%B0%D1%87%D0%BD%D0%B0_%D0%BE%D0%B4%D1%80%D0%B5%D0%B4%D0%BD%D0%B8%D1%86%D0%B0)',\n",
       " 'https://sh.wikipedia.org/wiki/Python',\n",
       " 'https://fi.wikipedia.org/wiki/Python',\n",
       " 'https://sv.wikipedia.org/wiki/Pyton',\n",
       " 'https://th.wikipedia.org/wiki/%E0%B9%84%E0%B8%9E%E0%B8%97%E0%B8%AD%E0%B8%99',\n",
       " 'https://tr.wikipedia.org/wiki/Python',\n",
       " 'https://uk.wikipedia.org/wiki/%D0%9F%D1%96%D1%84%D0%BE%D0%BD',\n",
       " 'https://ur.wikipedia.org/wiki/%D9%BE%D8%A7%D8%A6%DB%8C%D8%AA%DA%BE%D9%88%D9%86',\n",
       " 'https://vi.wikipedia.org/wiki/Python',\n",
       " 'https://zh.wikipedia.org/wiki/Python_(%E6%B6%88%E6%AD%A7%E4%B9%89)',\n",
       " 'https://www.wikidata.org/wiki/Special:EntityPage/Q747452#sitelinks-wikipedia',\n",
       " '//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License',\n",
       " '//creativecommons.org/licenses/by-sa/3.0/',\n",
       " '//foundation.wikimedia.org/wiki/Terms_of_Use',\n",
       " '//foundation.wikimedia.org/wiki/Privacy_policy',\n",
       " '//www.wikimediafoundation.org/',\n",
       " 'https://foundation.wikimedia.org/wiki/Privacy_policy',\n",
       " '/wiki/Wikipedia:About',\n",
       " '/wiki/Wikipedia:General_disclaimer',\n",
       " '//en.wikipedia.org/wiki/Wikipedia:Contact_us',\n",
       " 'https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute',\n",
       " 'https://stats.wikimedia.org/v2/#/en.wikipedia.org',\n",
       " 'https://foundation.wikimedia.org/wiki/Cookie_statement',\n",
       " '//en.m.wikipedia.org/w/index.php?title=Python&mobileaction=toggle_view_mobile',\n",
       " 'https://wikimediafoundation.org/',\n",
       " 'https://www.mediawiki.org/']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "def request(url):\n",
    "    raw = requests.get(url).text\n",
    "    return BeautifulSoup(raw, 'html.parser')\n",
    "\n",
    "def links(url):\n",
    "    beauty = request(url)\n",
    "    links = beauty.select('[href]')\n",
    "    return [link['href'] for link in links]\n",
    "links(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Titles that have changed in the United States Code since its last release point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'http://uscode.house.gov/download/download.shtml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'54'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "def titles(url):\n",
    "    beauty = request(url)\n",
    "    titles = beauty.select('div.uscitemlist div.uscitem:last-of-type')\n",
    "    last = [last.text.strip() for last in titles]\n",
    "    for element in last:\n",
    "        elem = element.split()\n",
    "        return elem[1]\n",
    "titles(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Python list with the top ten FBI's Most Wanted names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.fbi.gov/wanted/topten'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['JASON DEREK BROWN',\n",
       " 'ALEXIS FLORES',\n",
       " 'EUGENE PALMER',\n",
       " 'SANTIAGO VILLALBA MEDEROS',\n",
       " 'RAFAEL CARO-QUINTERO',\n",
       " 'ROBERT WILLIAM FISHER',\n",
       " 'BHADRESHKUMAR CHETANBHAI PATEL',\n",
       " 'ARNOLDO JIMENEZ',\n",
       " 'ALEJANDRO ROSALES CASTILLO',\n",
       " 'YASER ABDEL SAID']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "def wanted(url):\n",
    "    beauty = request(url)\n",
    "    names = beauty.select('h3 a')\n",
    "    return [name.text.strip() for name in names]\n",
    "wanted(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.emsc-csem.org/Earthquake/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code\n",
    "import re\n",
    "def earthquakes(tag):\n",
    "    #dates = beauty.select('b i + a')\n",
    "    #data = beauty.select('tr')\n",
    "    #tag = beauty.find_all('tbody')[0]\n",
    "    info = tag.find_all('td')\n",
    "    info = info[3::]\n",
    "    return {\n",
    "        \"date\": re.findall('\\d{4}\\-\\d{2}\\-\\d{2}',info[0].text.strip()),\n",
    "        \"time\": re.findall('\\d{2}\\:\\d{2}\\:\\d{2}',info[0].text.strip()),\n",
    "        \"lat\": (info[1].text) + info[2].text,\n",
    "        \"long\": (info[3].text) + info[4].text,\n",
    "        \"region\": info[8].text\n",
    "    }\n",
    "#info[0].text.strip()\n",
    "beauty = request(url)\n",
    "tabla = beauty.find_all('tbody')[0]\n",
    "earthquakes_dict = [earthquakes(fila) for fila in tabla.find_all(\"tr\")[1:]]\n",
    "df = pd.DataFrame(earthquakes_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the date, days, title, city, country of next 25 hackathon events as a Pandas dataframe table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://hackevents.co/hackathons'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no funciona la web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count number of tweets by a given Twitter account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to include a ***try/except block*** for account names not found. \n",
    "<br>***Hint:*** the program should count the number of tweets for any provided account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'48498'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "def tweets(username, url):\n",
    "    try:\n",
    "        web = url + username\n",
    "        beauty = request(web)\n",
    "        lista =  beauty.select('span.ProfileNav-value[data-is-compact = \"true\"]')\n",
    "        return lista[0]['data-count']\n",
    "    except:\n",
    "        return None\n",
    "tweets('realdonaldtrump',url)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of followers of a given twitter account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to include a ***try/except block*** in case account/s name not found. \n",
    "<br>***Hint:*** the program should count the followers for any provided account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'51736655'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "def followers(username, url):\n",
    "    try:\n",
    "        web = url + username\n",
    "        beauty = request(web)\n",
    "        lista =  beauty.select('span.ProfileNav-value[data-is-compact = \"true\"]')\n",
    "        if len(lista) > 1:\n",
    "            return lista[1]['data-count']\n",
    "        else: \n",
    "            return lista[0]['data-count']\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "followers('shakira',url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List all language names and number of related articles in the order they appear in wikipedia.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.wikipedia.org/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<small><bdi dir=\"ltr\">5 994 000+</bdi> <span>articles</span></small>, <small><bdi dir=\"ltr\">1 185 000+</bdi> <span>è¨äº</span></small>, <small><bdi dir=\"ltr\">2 385 000+</bdi> <span>Artikel</span></small>, <small><bdi dir=\"ltr\">1 571 000+</bdi> <span>artÃ­culos</span></small>, <small><bdi dir=\"ltr\">1 590 000+</bdi> <span>ÑÑÐ°ÑÐµÐ¹</span></small>, <small><bdi dir=\"ltr\">2 171 000+</bdi> <span>articles</span></small>, <small><bdi dir=\"ltr\">1 576 000+</bdi> <span>voci</span></small>, <small><bdi dir=\"ltr\">1 090 000+</bdi> <span data-convert-hans=\"æ¡ç®\" id=\"zh_art\">æ¢ç®</span></small>, <small><bdi dir=\"ltr\">1 018 000+</bdi> <span>artigos</span></small>, <small><bdi dir=\"ltr\">1 379 000+</bdi> <span>haseÅ</span></small>]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['English5\\xa0994\\xa0000+ articles',\n",
       " 'æ\\x97¥æ\\x9c¬èª\\x9e1\\xa0185\\xa0000+ è¨\\x98äº\\x8b',\n",
       " 'Deutsch2\\xa0385\\xa0000+ Artikel',\n",
       " 'EspaÃ±ol1\\xa0571\\xa0000+ artÃ\\xadculos',\n",
       " 'Ð\\xa0Ñ\\x83Ñ\\x81Ñ\\x81ÐºÐ¸Ð¹1\\xa0590\\xa0000+ Ñ\\x81Ñ\\x82Ð°Ñ\\x82ÐµÐ¹',\n",
       " 'FranÃ§ais2\\xa0171\\xa0000+ articles',\n",
       " 'Italiano1\\xa0576\\xa0000+ voci',\n",
       " 'ä¸\\xadæ\\x96\\x871\\xa0090\\xa0000+ æ¢\\x9dç\\x9b®',\n",
       " 'PortuguÃªs1\\xa0018\\xa0000+ artigos',\n",
       " 'Polski1\\xa0379\\xa0000+ haseÅ\\x82']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "def languages(url):\n",
    "    beauty = request(url)\n",
    "    languages = beauty.select('a strong')\n",
    "    articles = beauty.select('a small')\n",
    "    cremallera = zip(languages, articles)\n",
    "    print(articles)\n",
    "    return [language.text.strip() + article.text.strip() for (language,article) in cremallera]\n",
    "languages(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A list with the different kind of datasets available in data.gov.uk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://data.gov.uk/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Business and economy',\n",
       " 'Crime and justice',\n",
       " 'Defence',\n",
       " 'Education',\n",
       " 'Environment',\n",
       " 'Government',\n",
       " 'Government spending',\n",
       " 'Health',\n",
       " 'Mapping',\n",
       " 'Society',\n",
       " 'Towns and cities',\n",
       " 'Transport']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "def datasets(url):\n",
    "    beauty = request(url)\n",
    "    datasets = beauty.select('h2 a[href]')\n",
    "    return [dataset.text.strip() for dataset in datasets]\n",
    "datasets(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 languages by number of native speakers stored in a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mandarin Chinese',\n",
       " 'Sino-Tibetan',\n",
       " 'Sinitic',\n",
       " 'Spanish',\n",
       " 'Indo-European',\n",
       " 'Romance',\n",
       " 'English',\n",
       " 'Indo-European',\n",
       " 'Germanic',\n",
       " 'Hindi',\n",
       " 'Hindustani',\n",
       " '[9]',\n",
       " 'Indo-European',\n",
       " 'Indo-Aryan',\n",
       " 'Bengali',\n",
       " 'Indo-European',\n",
       " 'Indo-Aryan',\n",
       " 'Portuguese',\n",
       " 'Indo-European',\n",
       " 'Romance',\n",
       " 'Russian',\n",
       " 'Indo-European',\n",
       " 'Balto-Slavic',\n",
       " 'Japanese',\n",
       " 'Japonic',\n",
       " 'Japanese',\n",
       " 'Western Punjabi',\n",
       " '[10]',\n",
       " 'Indo-European',\n",
       " 'Indo-Aryan',\n",
       " 'Marathi',\n",
       " 'Indo-European',\n",
       " 'Indo-Aryan',\n",
       " 'Telugu',\n",
       " 'Dravidian',\n",
       " 'South-Central',\n",
       " 'Wu Chinese',\n",
       " 'Sino-Tibetan',\n",
       " 'Sinitic',\n",
       " 'Turkish',\n",
       " 'Turkic',\n",
       " 'Oghuz',\n",
       " 'Korean',\n",
       " 'Koreanic',\n",
       " 'language isolate',\n",
       " 'French',\n",
       " 'Indo-European',\n",
       " 'Romance',\n",
       " 'German',\n",
       " 'Indo-European',\n",
       " 'Germanic',\n",
       " 'Vietnamese',\n",
       " 'Austroasiatic',\n",
       " 'Vietic',\n",
       " 'Tamil',\n",
       " 'Dravidian',\n",
       " 'South',\n",
       " 'Yue Chinese',\n",
       " 'Sino-Tibetan',\n",
       " 'Sinitic',\n",
       " 'Urdu',\n",
       " 'Hindustani',\n",
       " '[9]',\n",
       " 'Indo-European',\n",
       " 'Indo-Aryan',\n",
       " 'Javanese',\n",
       " 'Austronesian',\n",
       " 'Malayo-Polynesian',\n",
       " 'Italian',\n",
       " 'Indo-European',\n",
       " 'Romance',\n",
       " 'Egyptian Arabic',\n",
       " 'Afroasiatic',\n",
       " 'Semitic',\n",
       " 'Gujarati',\n",
       " 'Indo-European',\n",
       " 'Indo-Aryan',\n",
       " 'Iranian Persian',\n",
       " 'Indo-European',\n",
       " 'Iranian',\n",
       " 'Bhojpuri',\n",
       " 'Indo-European',\n",
       " 'Indo-Aryan',\n",
       " 'Min Nan Chinese',\n",
       " 'Sino-Tibetan',\n",
       " 'Sinitic',\n",
       " 'Hakka Chinese',\n",
       " 'Sino-Tibetan',\n",
       " 'Sinitic',\n",
       " 'Jin Chinese',\n",
       " 'Sino-Tibetan',\n",
       " 'Sinitic',\n",
       " 'Hausa',\n",
       " 'Afroasiatic',\n",
       " 'Chadic',\n",
       " 'Kannada',\n",
       " 'Dravidian',\n",
       " 'South',\n",
       " 'Indonesian',\n",
       " 'Malay',\n",
       " 'Austronesian',\n",
       " 'Malayo-Polynesian',\n",
       " 'Polish',\n",
       " 'Indo-European',\n",
       " 'Balto-Slavic',\n",
       " 'Yoruba',\n",
       " 'Niger–Congo',\n",
       " 'Volta–Niger',\n",
       " 'Xiang Chinese',\n",
       " 'Sino-Tibetan',\n",
       " 'Sinitic',\n",
       " 'Malayalam',\n",
       " 'Dravidian',\n",
       " 'South',\n",
       " 'Odia',\n",
       " 'Indo-European',\n",
       " 'Indo-Aryan',\n",
       " 'Maithili',\n",
       " 'Indo-European',\n",
       " 'Indo-Aryan',\n",
       " 'Burmese',\n",
       " 'Sino-Tibetan',\n",
       " 'Lolo-Burmese',\n",
       " 'Eastern Punjabi',\n",
       " '[10]',\n",
       " 'Indo-European',\n",
       " 'Indo-Aryan',\n",
       " 'Sunda',\n",
       " 'Austronesian',\n",
       " 'Malayo-Polynesian',\n",
       " 'Sudanese Arabic',\n",
       " 'Afroasiatic',\n",
       " 'Semitic',\n",
       " 'Algerian Arabic',\n",
       " 'Afroasiatic',\n",
       " 'Semitic',\n",
       " 'Moroccan Arabic',\n",
       " 'Afroasiatic',\n",
       " 'Semitic',\n",
       " 'Ukrainian',\n",
       " 'Indo-European',\n",
       " 'Balto-Slavic',\n",
       " 'Igbo',\n",
       " 'Niger–Congo',\n",
       " 'Volta–Niger',\n",
       " 'Northern Uzbek',\n",
       " 'Turkic',\n",
       " 'Karluk',\n",
       " 'Sindhi',\n",
       " 'Indo-European',\n",
       " 'Indo-Aryan',\n",
       " 'North Levantine Arabic',\n",
       " 'Afroasiatic',\n",
       " 'Semitic',\n",
       " 'Romanian',\n",
       " 'Indo-European',\n",
       " 'Romance',\n",
       " 'Tagalog',\n",
       " 'Austronesian',\n",
       " 'Malayo-Polynesian',\n",
       " 'Dutch',\n",
       " 'Indo-European',\n",
       " 'Germanic',\n",
       " 'Saʽidi Arabic',\n",
       " 'Afroasiatic',\n",
       " 'Semitic',\n",
       " 'Gan Chinese',\n",
       " 'Sino-Tibetan',\n",
       " 'Sinitic',\n",
       " 'Amharic',\n",
       " 'Afroasiatic',\n",
       " 'Semitic',\n",
       " 'Northern Pashto',\n",
       " 'Indo-European',\n",
       " 'Iranian',\n",
       " 'Magahi',\n",
       " 'Indo-European',\n",
       " 'Indo-Aryan',\n",
       " 'Thai',\n",
       " 'Kra–Dai',\n",
       " 'Tai',\n",
       " 'Saraiki',\n",
       " 'Indo-European',\n",
       " 'Indo-Aryan',\n",
       " 'Khmer',\n",
       " 'Austroasiatic',\n",
       " 'Khmer',\n",
       " 'Chhattisgarhi',\n",
       " 'Indo-European',\n",
       " 'Indo-Aryan',\n",
       " 'Somali',\n",
       " 'Afroasiatic',\n",
       " 'Cushitic',\n",
       " 'Malay',\n",
       " 'Malay',\n",
       " 'Austronesian',\n",
       " 'Malayo-Polynesian',\n",
       " 'Cebuano',\n",
       " 'Austronesian',\n",
       " 'Malayo-Polynesian',\n",
       " 'Nepali',\n",
       " 'Indo-European',\n",
       " 'Indo-Aryan',\n",
       " 'Mesopotamian Arabic',\n",
       " 'Afroasiatic',\n",
       " 'Semitic',\n",
       " 'Assamese',\n",
       " 'Indo-European',\n",
       " 'Indo-Aryan',\n",
       " 'Sinhala',\n",
       " 'Indo-European',\n",
       " 'Indo-Aryan',\n",
       " 'Northern Kurdish',\n",
       " 'Indo-European',\n",
       " 'Iranian',\n",
       " 'Hejazi Arabic',\n",
       " 'Afroasiatic',\n",
       " 'Semitic',\n",
       " 'Nigerian Fulfulde',\n",
       " 'Niger–Congo',\n",
       " 'Senegambian',\n",
       " 'Bavarian',\n",
       " 'Indo-European',\n",
       " 'Germanic',\n",
       " 'South Azerbaijani',\n",
       " 'Turkic',\n",
       " 'Oghuz',\n",
       " 'Greek',\n",
       " 'Indo-European',\n",
       " 'Hellenic',\n",
       " 'Chittagonian',\n",
       " 'Indo-European',\n",
       " 'Indo-Aryan',\n",
       " 'Kazakh',\n",
       " 'Turkic',\n",
       " 'Kipchak',\n",
       " 'Deccan',\n",
       " 'Indo-European',\n",
       " 'Indo-Aryan',\n",
       " 'Hungarian',\n",
       " 'Uralic',\n",
       " 'Ugric',\n",
       " 'Kinyarwanda',\n",
       " 'Niger–Congo',\n",
       " 'Bantu',\n",
       " 'Zulu',\n",
       " 'Niger–Congo',\n",
       " 'Bantu',\n",
       " 'South Levantine Arabic',\n",
       " 'Afroasiatic',\n",
       " 'Semitic',\n",
       " 'Tunisian Arabic',\n",
       " 'Afroasiatic',\n",
       " 'Semitic',\n",
       " 'Sanaani Spoken Arabic',\n",
       " 'Afroasiatic',\n",
       " 'Semitic',\n",
       " 'Min Bei Chinese',\n",
       " 'Sino-Tibetan',\n",
       " 'Sinitic',\n",
       " 'Southern Pashto',\n",
       " 'Indo-European',\n",
       " 'Iranian',\n",
       " 'Rundi',\n",
       " 'Niger–Congo',\n",
       " 'Bantu',\n",
       " 'Czech',\n",
       " 'Indo-European',\n",
       " 'Balto-Slavic',\n",
       " 'Taʽizzi-Adeni Arabic',\n",
       " 'Afroasiatic',\n",
       " 'Semitic',\n",
       " 'Uyghur',\n",
       " 'Turkic',\n",
       " 'Karluk',\n",
       " 'Min Dong Chinese',\n",
       " 'Sino-Tibetan',\n",
       " 'Sinitic',\n",
       " 'Sylheti',\n",
       " 'Indo-European',\n",
       " 'Indo-Aryan',\n",
       " 'Mandarin',\n",
       " 'Spanish',\n",
       " 'English',\n",
       " 'Hindi',\n",
       " '[a]',\n",
       " 'Arabic',\n",
       " 'Portuguese',\n",
       " 'Bengali',\n",
       " 'Russian',\n",
       " 'Japanese',\n",
       " 'Punjabi',\n",
       " 'German',\n",
       " 'Javanese',\n",
       " 'Wu',\n",
       " 'Shanghainese',\n",
       " 'Malay',\n",
       " 'Indonesian',\n",
       " 'Malaysian',\n",
       " 'Telugu',\n",
       " 'Vietnamese',\n",
       " 'Korean',\n",
       " 'French',\n",
       " 'Marathi',\n",
       " 'Tamil',\n",
       " 'Urdu',\n",
       " 'Turkish',\n",
       " 'Italian',\n",
       " 'Yue',\n",
       " 'Cantonese',\n",
       " 'Thai',\n",
       " 'Gujarati',\n",
       " 'Jin',\n",
       " 'Southern Min',\n",
       " 'Hokkien',\n",
       " 'Teochew',\n",
       " 'Persian',\n",
       " 'Polish',\n",
       " 'Pashto',\n",
       " 'Kannada',\n",
       " 'Xiang',\n",
       " 'Malayalam',\n",
       " 'Sundanese',\n",
       " 'Hausa',\n",
       " 'Odia',\n",
       " 'Burmese',\n",
       " 'Hakka',\n",
       " 'Ukrainian',\n",
       " 'Bhojpuri',\n",
       " '[b]',\n",
       " 'Tagalog',\n",
       " 'Filipino',\n",
       " 'Yoruba',\n",
       " 'Maithili',\n",
       " '[b]',\n",
       " 'Uzbek',\n",
       " 'Sindhi',\n",
       " 'Amharic',\n",
       " 'Fula',\n",
       " 'Romanian',\n",
       " 'Oromo',\n",
       " 'Igbo',\n",
       " 'Azerbaijani',\n",
       " 'Awadhi',\n",
       " '[b]',\n",
       " 'Gan',\n",
       " 'Cebuano',\n",
       " 'Dutch',\n",
       " 'Kurdish',\n",
       " 'Serbo-Croatian',\n",
       " 'Malagasy',\n",
       " 'Saraiki',\n",
       " '[c]',\n",
       " 'Nepali',\n",
       " 'Sinhala',\n",
       " 'Chittagonian',\n",
       " 'Zhuang',\n",
       " 'Khmer',\n",
       " 'Turkmen',\n",
       " 'Assamese',\n",
       " 'Madurese',\n",
       " 'Somali',\n",
       " 'Marwari',\n",
       " '[b]',\n",
       " 'Magahi',\n",
       " '[b]',\n",
       " 'Haryanvi',\n",
       " '[b]',\n",
       " 'Hungarian',\n",
       " 'Chhattisgarhi',\n",
       " '[b]',\n",
       " 'Greek',\n",
       " 'Chewa',\n",
       " 'Deccan',\n",
       " 'Akan',\n",
       " 'Kazakh',\n",
       " 'Northern Min',\n",
       " 'disputed',\n",
       " 'discuss',\n",
       " 'Sylheti',\n",
       " 'Zulu',\n",
       " 'Czech',\n",
       " 'Kinyarwanda',\n",
       " 'Dhundhari',\n",
       " '[b]',\n",
       " 'Haitian Creole',\n",
       " 'Eastern Min',\n",
       " 'Fuzhou dialect',\n",
       " 'Ilocano',\n",
       " 'Quechua',\n",
       " 'Kirundi',\n",
       " 'Swedish',\n",
       " 'Hmong',\n",
       " 'Shona',\n",
       " 'Uyghur',\n",
       " 'Hiligaynon/Ilonggo',\n",
       " 'Mossi',\n",
       " 'Xhosa',\n",
       " 'Belarusian',\n",
       " '[d]',\n",
       " 'Balochi',\n",
       " 'Konkani',\n",
       " 'Countries by spoken languages',\n",
       " 'Official',\n",
       " 'Countries by the number of recognized official languages',\n",
       " 'Arabic',\n",
       " 'Chinese',\n",
       " 'Dutch/Afrikaans',\n",
       " 'English',\n",
       " 'French',\n",
       " 'German',\n",
       " 'Malay',\n",
       " 'Portuguese',\n",
       " 'Romanian',\n",
       " 'Russian',\n",
       " 'Spanish',\n",
       " 'Tamil',\n",
       " 'Endonyms',\n",
       " 'Countries and capitals in native languages',\n",
       " 'Exonyms',\n",
       " 'Country names in various languages',\n",
       " 'A–C',\n",
       " 'D–I',\n",
       " 'J–P',\n",
       " 'Q–Z',\n",
       " 'Languages of the European Union',\n",
       " 'Africa',\n",
       " 'Americas',\n",
       " 'North',\n",
       " 'South',\n",
       " 'Asia',\n",
       " 'East',\n",
       " 'South',\n",
       " 'Europe',\n",
       " 'Oceania',\n",
       " 'Official languages',\n",
       " 'by state',\n",
       " 'List of languages without official status',\n",
       " 'Countries by the number of recognized official languages',\n",
       " 'Languages by the number of countries in which they are recognized as an official language',\n",
       " 'By number of total speakers',\n",
       " 'Languages in censuses',\n",
       " 'family',\n",
       " 'Language families',\n",
       " 'List of Afro-Asiatic languages',\n",
       " 'List of Austronesian languages',\n",
       " 'List of Indo-European languages',\n",
       " 'List of Mongolic languages',\n",
       " 'List of Tungusic languages',\n",
       " 'List of Turkic languages',\n",
       " 'List of Uralic languages',\n",
       " 'geopolitical',\n",
       " 'Arab League (Arabic)',\n",
       " 'Dutch Language Union (Dutch)',\n",
       " 'English Speaking Union',\n",
       " 'Francophonie (French)',\n",
       " 'Community of Portuguese Language Countries (Portuguese)',\n",
       " 'Países Africanos de Língua Oficial Portuguesa (Portuguese)',\n",
       " 'Latin Union',\n",
       " 'Romance',\n",
       " 'Hispanidad (Spanish)',\n",
       " 'Turkic Council',\n",
       " 'Turkic',\n",
       " 'International Organization of Turkic Culture',\n",
       " 'Turkic',\n",
       " 'Three Linguistic Spaces (French, Portuguese and Spanish)',\n",
       " 'Lists of languages',\n",
       " 'Category:Languages']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "def langs(url):\n",
    "    beauty = request(url)\n",
    "    langs = beauty.select('tbody tr td a[href]')\n",
    "    return [lang.text.strip() for lang in langs]\n",
    "langs(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape a certain number of tweets of a given Twitter account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GAME OVER!pic.twitter.com/yvMa6bPqfy',\n",
       " \"Can't violate the Logan Act when you're working for the president or acting at his request. \\n\\nLogan Act was created to block private citizens from acting as *if* they represented (the President of) the USA (foreign policy specifically). \\nSuch as Kerry talking to Iran after 2016.\",\n",
       " 'CNN\\'s John King: \"Republicans make a good point. The Whistleblower started all this. Why has the Whistleblower never been questioned? Shouldn’t the House Democrats have found a secure way to do that? It’s a legitimate point of debate.\"pic.twitter.com/lsutmBXBwJ',\n",
       " 'BREAKING: The 1 witness transcript Schiff won\\'t release (out of 18) talks about Schiff and the \"whistleblower\"--and how the inspector general (an Obama holdover) who facilitated the \"whistleblower,\" failed to investigate prior contacts between the \"whistleblower\" & Schiff\\'s staff',\n",
       " 'There is one transcript that Adam Schiff refuses to release.\\n\\nThe Intelligence Community Inspector General transcript. \\n\\nIt needs to be released for the American people to read.\\n\\nWhy? Bc it answers ?s on the coordination & political bias of the whistleblower #ReleasetheTranscript',\n",
       " 'My question today is about whether or not individuals who were holdovers from the Obama National Security Council and Democrat partisans conspired with Schiff staffers to plot impeaching the President before there were formal House impeachment proceedings.',\n",
       " 'WATCH THIShttps://twitter.com/WhiteHouse/status/1222947025928577024\\xa0…',\n",
       " 'BREAKING: Over dozen Senators just asked Schiff about RCI story (below) & he did not deny it but complained about “attacks\" & \"smears on my staff.\" Schiff refused to answer direct question why he hired WB pal Misko day after the Trump-Ukraine phone call:https://www.realclearinvestigations.com/articles/2020/01/22/whistleblower_was_overheard_in_17_discussing_with_ally_how_to_remove_trump_121701.html\\xa0…',\n",
       " 'While Democrats focus on WITCH HUNT, @realDonaldTrump is highlighting how he is working to help American autoworkers and American manufacturing!https://twitter.com/TeamTrump/status/1223000462460407808\\xa0…',\n",
       " 'Trump administration forms #coronavirus task forcehttps://www.theepochtimes.com/trump-administration-forms-coronavirus-task-force_3221590.html\\xa0…',\n",
       " 'Schiff hasn’t done a thing for his district nor has Nancy Pelosi in SF!\\n\\nNancy has been in power for YEARS and San Francisco is suffering from a homeless problem that only gets worse.\\n\\nWhat does Nancy focus on?\\n\\nImpeachment!\\n\\nShe doesn’t care about her own district! #WheresNancy',\n",
       " 'Schiff is a congenital liar that has done nothing positive for the American people.',\n",
       " \"If you're a Democrat you really have no choice but to #VoteTrump2020 . He's the only guy in Washington doing anything & they're trying to remove him. They want put the USA in debt TRILLIONS, he wants to lower prescription costs. The economy is too good don't screw it up.pic.twitter.com/JaB6q6akgE\",\n",
       " \"Schiff and many more of these elected people should all be ashamed of themselves. What have they done to help people in their districts or the American people? From what I've seen all they have focused on is trying to frame & impeach President Trump & cover up crimes of Democratspic.twitter.com/CxwEecqKG1\",\n",
       " 'When will Schiff be investigated?',\n",
       " 'While Dems obsess over impeachment, @realDonaldTrump is working for YOU!\\n\\n– #USMCA signed\\n– Middle East peace plan announced\\n– Gorsuch/Kavanaugh confirmed\\n– Taxes cut\\n– 7 MILLION jobs added\\n– Unemployment at a 50-year low\\n– Stronger military\\n– Terrorists killed\\n– Stronger borderpic.twitter.com/hPo5vzMUNX',\n",
       " \"Democrats didn't accept the results of the 2016 election; that's why they've been pushing to impeach and remove President @realDonaldTrump from office.\\n\\nAnd now, they won't accept the expected acquittal of President Trump based on the case they submitted to the Senate.\",\n",
       " \"Our defense team has done its job. We've shown Adam Schiff & Co. have no case, no evidence, and no justification for impeaching a duly elected president. There's only one way this #ShamImpeachment ends -- with the full (and likely bipartisan) acquittal of @realDonaldTrump.pic.twitter.com/c8ilJvzU95\",\n",
       " '128 days. 17 Democrat witnesses. And they still have no case.\\n\\nIt’s over. This trial should end tomorrow with President Trump ACQUITTED.',\n",
       " 'How about this:\\n\\nIf the Senate discusses calling witnesses, and Adam Schiff continues demanding we block questions and testimony on the whistleblower... then Adam Schiff should testify',\n",
       " 'Once again Adam Schiff makes up dialogue of the President of the United States on the floor of the Senate.\\n\\nJust like Schiff’s made-up transcript that he claimed was just “parody.” \\n\\nThis is not SNL. \\n\\nThis is an impeachment trial that Schiff has shamefully turned into a parody.']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "def tweet_content(username, url):\n",
    "    try:\n",
    "        web = url + username\n",
    "        beauty = request(web)\n",
    "        return [message.text.strip() for message in beauty.select('div p[lang]')]\n",
    "    except:\n",
    "        return None\n",
    "tweet_content('realdonaldtrump',url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMDB's Top 250 data (movie name, Initial release, director name and stars) as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url = 'https://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', 'Cadena perpetua']"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code\n",
    "beauty = request(url)\n",
    "tabla = beauty.find_all('tbody')[0]\n",
    "#tabla = beauty.select('tbody.lister-list tr')\n",
    "def movies_(tag):\n",
    "    movie = tag.select('td a[href]')\n",
    "    return [mov.text.strip() for mov in movie]\n",
    "\n",
    "movies = [movies_(fila) for fila in tabla.find_all('tr')[1:]]\n",
    "movies_(tabla.find_all('tr')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Movie name, year and a brief summary of the top 10 random movies (IMDB) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the url you will scrape in this exercise\n",
    "url = 'http://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the live weather report (temperature, wind speed, description and weather) of a given city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    884\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-784a0b919997>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#https://openweathermap.org/current\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Enter the city:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'http://api.openweathermap.org/data/2.5/weather?'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'q='\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcity\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'&APPID=b35975e18dc93725acb092f7272cc6b8&units=metric'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    861\u001b[0m         )\n\u001b[1;32m    862\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    888\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    891\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#https://openweathermap.org/current\n",
    "city = city=input('Enter the city:')\n",
    "url = 'http://api.openweathermap.org/data/2.5/weather?'+'q='+city+'&APPID=b35975e18dc93725acb092f7272cc6b8&units=metric'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Book name,price and stock availability as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise. \n",
    "# It is a fictional bookstore created to be scraped. \n",
    "url = 'http://books.toscrape.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
